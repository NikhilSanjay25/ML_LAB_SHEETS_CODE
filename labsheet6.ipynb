{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ac21299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target_names[iris.target] # This line creates the column!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7617d4",
   "metadata": {},
   "source": [
    "# A1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc5e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entropy of the Iris dataset target: 1.5850\n"
     ]
    }
   ],
   "source": [
    "def calculate_entropy(target_col):\n",
    "    \"\"\"\n",
    "    Calculates the entropy for a given target column (pandas Series).\n",
    "    \n",
    "    Parameters:\n",
    "    - target_col (pd.Series): The data column for which to calculate entropy.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The entropy value.\n",
    "    \"\"\"\n",
    "    # Find unique values and their counts\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = counts / len(target_col)\n",
    "    \n",
    "    # Calculate entropy using the formula\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Calculate the entropy of the whole dataset's target variable\n",
    "total_entropy = calculate_entropy(df['target'])\n",
    "print(f\"\\nEntropy of the Iris dataset target: {total_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cc137",
   "metadata": {},
   "source": [
    "# A2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94701b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "def func2(dataset):\n",
    "    df = pd.read_csv(dataset)\n",
    "    count = df['Species'].value_counts()\n",
    "    probs = count/len(df)\n",
    "    op = np.sum(1-probs**2)\n",
    "    return op\n",
    "\n",
    "f = func2(\"Iris.csv\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2880c",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5428ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best feature for the root node is: 'petal width (cm)'\n"
     ]
    }
   ],
   "source": [
    "def find_best_split(df, target_col_name):\n",
    "    \"\"\"\n",
    "    Finds the best feature to split on by maximizing Information Gain.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataset (or subset) to split.\n",
    "    - target_col_name (str): The name of the target variable column.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The name of the feature that provides the highest information gain.\n",
    "    \"\"\"\n",
    "    # Calculate the entropy of the current dataset (before splitting)\n",
    "    total_entropy = calculate_entropy(df[target_col_name])\n",
    "    \n",
    "    # Get feature names (all columns except the target)\n",
    "    features = [col for col in df.columns if col != target_col_name]\n",
    "    \n",
    "    max_info_gain = -1\n",
    "    best_feature = None\n",
    "    \n",
    "    # Iterate through each feature to calculate its information gain\n",
    "    for feature in features:\n",
    "        weighted_entropy = 0\n",
    "        # For each unique value in the feature, calculate its weighted entropy\n",
    "        for value in df[feature].unique():\n",
    "            subset = df[df[feature] == value]\n",
    "            subset_proportion = len(subset) / len(df)\n",
    "            subset_entropy = calculate_entropy(subset[target_col_name])\n",
    "            weighted_entropy += subset_proportion * subset_entropy\n",
    "            \n",
    "        # Calculate the information gain for the current feature\n",
    "        info_gain = total_entropy - weighted_entropy\n",
    "        \n",
    "        # Update the best feature if this one is better\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature\n",
    "\n",
    "# --- Example Usage ---\n",
    "# We use the binned dataframe to find the root node\n",
    "root_node_feature = find_best_split(binned_df, 'target')\n",
    "print(f\"\\nBest feature for the root node is: '{root_node_feature}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0184ec19",
   "metadata": {},
   "source": [
    "# A4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c2da963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binned Dataset (first 5 rows):\n",
      "  sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)  \\\n",
      "0                 0                2                 0                0   \n",
      "1                 0                1                 0                0   \n",
      "2                 0                2                 0                0   \n",
      "3                 0                1                 0                0   \n",
      "4                 0                2                 0                0   \n",
      "\n",
      "   target  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n"
     ]
    }
   ],
   "source": [
    "def custom_binning(feature_col, num_bins=4, method='equal_width'):\n",
    "    \"\"\"\n",
    "    Manually implements binning for a continuous feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_col (pd.Series): The continuous data to bin.\n",
    "    - num_bins (int): The number of bins to create (default is 4).\n",
    "    - method (str): 'equal_width' or 'equal_frequency' (default is 'equal_width').\n",
    "    \n",
    "    Returns:\n",
    "    - pd.Series: The binned data as a series of strings.\n",
    "    \"\"\"\n",
    "    if method == 'equal_width':\n",
    "        min_val = feature_col.min()\n",
    "        max_val = feature_col.max()\n",
    "        bin_width = (max_val - min_val) / num_bins\n",
    "        # Define the edges of the bins\n",
    "        bins = np.arange(min_val, max_val, bin_width)\n",
    "        bins = np.append(bins, max_val * (1 + 1e-6)) # Add a small buffer to include the max value\n",
    "        \n",
    "        # Assign each data point to a bin\n",
    "        binned_data = np.digitize(feature_col, bins=bins, right=False)\n",
    "        # Convert from 1-based to 0-based index\n",
    "        return pd.Series(binned_data - 1, index=feature_col.index).astype(str)\n",
    "        \n",
    "    elif method == 'equal_frequency':\n",
    "        # Find bin edges using quantiles\n",
    "        bin_edges = pd.qcut(feature_col, q=num_bins, retbins=True, duplicates='drop')[1]\n",
    "        \n",
    "        # Assign each data point to a bin\n",
    "        binned_data = np.digitize(feature_col, bins=bin_edges, right=False)\n",
    "        binned_data[feature_col == feature_col.min()] = 1 # Ensure min value is in the first bin\n",
    "        # Convert from 1-based to 0-based index\n",
    "        return pd.Series(binned_data - 1, index=feature_col.index).astype(str)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'equal_width' or 'equal_frequency'\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Let's create a binned version of our dataframe for the next steps\n",
    "binned_df = pd.DataFrame()\n",
    "for col in iris.feature_names:\n",
    "    binned_df[col] = custom_binning(df[col], num_bins=4, method='equal_width')\n",
    "binned_df['target'] = df['target']\n",
    "\n",
    "print(\"\\nBinned Dataset (first 5 rows):\")\n",
    "print(binned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c7c2a",
   "metadata": {},
   "source": [
    "# A5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838c3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Decision Tree (dictionary format):\n",
      "{\n",
      "    \"petal width (cm)\": {\n",
      "        \"0\": \"setosa\",\n",
      "        \"2\": {\n",
      "            \"petal length (cm)\": {\n",
      "                \"2\": {\n",
      "                    \"sepal length (cm)\": {\n",
      "                        \"2\": {\n",
      "                            \"sepal width (cm)\": {\n",
      "                                \"2\": \"versicolor\",\n",
      "                                \"1\": \"versicolor\",\n",
      "                                \"0\": \"versicolor\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"1\": {\n",
      "                            \"sepal width (cm)\": {\n",
      "                                \"0\": \"versicolor\",\n",
      "                                \"1\": \"versicolor\",\n",
      "                                \"2\": \"versicolor\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"0\": \"virginica\"\n",
      "                    }\n",
      "                },\n",
      "                \"1\": \"versicolor\",\n",
      "                \"3\": \"virginica\"\n",
      "            }\n",
      "        },\n",
      "        \"1\": \"versicolor\",\n",
      "        \"3\": \"virginica\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def build_decision_tree(df, target_col_name, features):\n",
    "    \"\"\"\n",
    "    Recursively builds a decision tree from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The training data.\n",
    "    - target_col_name (str): The name of the target column.\n",
    "    - features (list): A list of feature names to consider for splitting.\n",
    "    \n",
    "    Returns:\n",
    "    - dict or value: A nested dictionary representing the tree.\n",
    "    \"\"\"\n",
    "    target_values = df[target_col_name]\n",
    "    \n",
    "    # --- Base Cases (Stopping Conditions) ---\n",
    "    # 1. If all target values are the same, return that value (pure node).\n",
    "    if len(target_values.unique()) == 1:\n",
    "        return target_values.unique()[0]\n",
    "        \n",
    "    # 2. If there are no more features left to split on, return the majority class.\n",
    "    if len(features) == 0:\n",
    "        return target_values.mode()[0]\n",
    "        \n",
    "    # --- Recursive Step ---\n",
    "    # Find the best feature to split on for the current dataset\n",
    "    best_feature = find_best_split(df[[*features, target_col_name]], target_col_name)\n",
    "    \n",
    "    # If no feature provides any information gain, stop and return the majority class.\n",
    "    if best_feature is None:\n",
    "        return target_values.mode()[0]\n",
    "        \n",
    "    # Create the tree structure as a nested dictionary\n",
    "    tree = {best_feature: {}}\n",
    "    \n",
    "    # Get remaining features for the next recursive call\n",
    "    remaining_features = [f for f in features if f != best_feature]\n",
    "    \n",
    "    # For each unique value of the best feature, create a new branch\n",
    "    for value in df[best_feature].unique():\n",
    "        subset = df[df[best_feature] == value]\n",
    "        # Recursively build the subtree for this branch\n",
    "        subtree = build_decision_tree(subset, target_col_name, remaining_features)\n",
    "        tree[best_feature][value] = subtree\n",
    "        \n",
    "    return tree\n",
    "\n",
    "# --- Example Usage ---\n",
    "features = list(binned_df.columns)\n",
    "features.remove('target')\n",
    "iris_tree = build_decision_tree(binned_df, 'target', features)\n",
    "\n",
    "# Print the generated tree (it's a nested dictionary)\n",
    "import json\n",
    "print(\"\\nGenerated Decision Tree (dictionary format):\")\n",
    "print(json.dumps(iris_tree, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f98e02",
   "metadata": {},
   "source": [
    "# A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40b0333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "petal width (cm) is '0' -> Class: setosa\n",
      "petal width (cm) is '2'|   petal length (cm) is '2'|   |   sepal length (cm) is '2'|   |   |   sepal width (cm) is '2' -> Class: versicolor\n",
      "|   |   |   sepal width (cm) is '1' -> Class: versicolor\n",
      "|   |   |   sepal width (cm) is '0' -> Class: versicolor\n",
      "|   |   sepal length (cm) is '1'|   |   |   sepal width (cm) is '0' -> Class: versicolor\n",
      "|   |   |   sepal width (cm) is '1' -> Class: versicolor\n",
      "|   |   |   sepal width (cm) is '2' -> Class: versicolor\n",
      "|   |   sepal length (cm) is '0' -> Class: virginica\n",
      "|   petal length (cm) is '1' -> Class: versicolor\n",
      "|   petal length (cm) is '3' -> Class: virginica\n",
      "petal width (cm) is '1' -> Class: versicolor\n",
      "petal width (cm) is '3' -> Class: virginica\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, indent=\"\"):\n",
    "    \"\"\"\n",
    "    Prints a text-based representation of a decision tree.\n",
    "\n",
    "    Parameters:\n",
    "    - tree (dict or value): The nested dictionary representing the tree.\n",
    "    - indent (str): The string used for indenting child nodes.\n",
    "    \"\"\"\n",
    "    # Base case: If the node is not a dictionary, it's a leaf node (a prediction).\n",
    "    if not isinstance(tree, dict):\n",
    "        print(f\" -> Class: {tree}\")\n",
    "        return\n",
    "\n",
    "    # It's a decision node. Get the feature name and the branches.\n",
    "    feature_name, branches = list(tree.items())[0]\n",
    "\n",
    "    # Recursively print each branch\n",
    "    for value, subtree in branches.items():\n",
    "        print(f\"{indent}{feature_name} is '{value}'\", end=\"\")\n",
    "        print_tree(subtree, indent + \"|   \")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming you have already built your 'iris_tree' dictionary from the previous step.\n",
    "# If not, you need to run the code that generates it first.\n",
    "#\n",
    "# iris_tree = build_decision_tree(binned_df, 'target', features)\n",
    "\n",
    "print(\"Decision Tree Structure:\")\n",
    "print_tree(iris_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81536d5b",
   "metadata": {},
   "source": [
    "# A7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fc4ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# --- Step 1: Corrected prediction function ---\n",
    "def predict_single(query, tree, default=None):\n",
    "    \"\"\"\n",
    "    Predicts the class for a single instance (a dictionary).\n",
    "    This version is robust against unseen data combinations.\n",
    "    \"\"\"\n",
    "    # Start at the root of the current subtree\n",
    "    feature = list(tree.keys())[0]\n",
    "    value = query.get(feature)\n",
    "\n",
    "    # If the feature value doesn't exist as a branch, return the default\n",
    "    if value not in tree[feature]:\n",
    "        return default\n",
    "\n",
    "    # Traverse to the next node in the tree\n",
    "    result = tree[feature][value]\n",
    "\n",
    "    # If we've reached another decision node, recurse\n",
    "    if isinstance(result, dict):\n",
    "        return predict_single(query, result, default)\n",
    "    # If we've reached a leaf, it's our prediction\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "# --- Step 2: Updated function to plot the boundary ---\n",
    "def visualize_decision_boundary(df, target_col, num_bins=4):\n",
    "    \"\"\"\n",
    "    Builds a tree on the first two features and visualizes its boundary.\n",
    "    \"\"\"\n",
    "    f1, f2 = df.columns[0], df.columns[1]\n",
    "    X_plot = df[[f1, f2]]\n",
    "    \n",
    "    y_map = {name: i for i, name in enumerate(df[target_col].unique())}\n",
    "    y_plot = df[target_col].map(y_map)\n",
    "    \n",
    "    # --- Build a 2D tree on binned data ---\n",
    "    binned_f1 = custom_binning(X_plot[f1], num_bins=num_bins)\n",
    "    binned_f2 = custom_binning(X_plot[f2], num_bins=num_bins)\n",
    "    train_df = pd.DataFrame({f1: binned_f1, f2: binned_f2, 'target': df[target_col]})\n",
    "    tree_2d = build_decision_tree(train_df, 'target', [f1, f2])\n",
    "\n",
    "    # ADDED: Determine the majority class to use as a default for unknown points\n",
    "    default_prediction = df[target_col].mode()[0]\n",
    "\n",
    "    # --- Create a mesh grid for the plot background ---\n",
    "    x_min, x_max = X_plot[f1].min() - 0.5, X_plot[f1].max() + 0.5\n",
    "    y_min, y_max = X_plot[f2].min() - 0.5, X_plot[f2].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # --- Predict class for each point on the grid ---\n",
    "    grid_f1 = custom_binning(pd.Series(xx.ravel(), name=f1), num_bins=num_bins)\n",
    "    grid_f2 = custom_binning(pd.Series(yy.ravel(), name=f2), num_bins=num_bins)\n",
    "    grid_df = pd.DataFrame({f1: grid_f1, f2: grid_f2})\n",
    "\n",
    "    # CHANGED: Pass the default_prediction to the robust predict_single function\n",
    "    predictions = grid_df.apply(\n",
    "        lambda row: predict_single(row.to_dict(), tree_2d, default=default_prediction), \n",
    "        axis=1\n",
    "    )\n",
    "    Z = predictions.map(y_map).values.reshape(xx.shape)\n",
    "\n",
    "    # --- Plot the results ---\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X_plot[f1], X_plot[f2], c=y_plot, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\n",
    "    plt.title(f\"Decision Boundary for '{f1}' vs '{f2}'\")\n",
    "    plt.xlabel(f1)\n",
    "    plt.ylabel(f2)\n",
    "    plt.show()\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Ensure you have run all previous code blocks to define df, custom_binning, etc.\n",
    "# print(\"\\nGenerating decision boundary plot for the first two features...\")\n",
    "# visualize_decision_boundary(df, 'target')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
